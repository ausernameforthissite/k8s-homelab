---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: record-k8s-kubelet-pleg
  labels:
    prometheus.io/instance: main
    instance.prometheus.io/main: enable
    instance.prometheus.io/prompp: enable
spec:
  groups:
  - name: record-k8s-kubelet-pleg
    rules:
    # scrape_interval == rule_evaluation_interval == 10s, therefore irate does not lose any samples
    # `irate[1m]` is much more precise than `rate[25s]`
    - record: kubelet_pleg_relist_duration_seconds:quantile_irate
      expr: |-
        histogram_quantile(0.99, irate(kubelet_pleg_relist_duration_seconds_bucket[1m]))
      labels:
        quantile: '0.99'
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: record-k8s-node-namespace
  labels:
    prometheus.io/instance: main
    instance.prometheus.io/main: enable
    instance.prometheus.io/prompp: enable
spec:
  groups:
  - name: record-k8s-node-namespace
    # using max by(container) should fix overlapping data on container restarts
    rules:
    - record: node_namespace:container_cpu_usage_seconds:irate
      expr: |-
        sum by (cluster_type, cluster, node, namespace) (
          max by(cluster_type, cluster, node, namespace, pod, container) (
            irate(container_cpu_usage_seconds_total{}[40s])
          )
        )
    - record: node_namespace:container_memory_usage_bytes
      expr: |-
        sum by (cluster_type, cluster, node, namespace) (
          max by(cluster_type, cluster, node, namespace, pod, container) (
            container_memory_usage_bytes{}
          )
        )
    - record: node_namespace:container_memory_rss
      expr: |-
        sum by (cluster_type, cluster, node, namespace) (
          max by(cluster_type, cluster, node, namespace, pod, container) (
            container_memory_rss{}
          )
        )
    - record: node_namespace:container_network_receive_bytes:irate
      expr: |-
        sum by (cluster_type, cluster, node, namespace) (
          sum by(cluster_type, cluster, node, namespace, pod) (
            max by(cluster_type, cluster, node, namespace, pod, container) (
              irate(container_network_receive_bytes_total{}[40s])
            )
          )
          * on(cluster_type, cluster, node, namespace, pod)
          # pods with hostNetwork have node network statistics, it's meaningless
          kube_pod_info{host_network="false"}
        )
    - record: node_namespace:container_network_transmit_bytes:irate
      expr: |-
        sum by (cluster_type, cluster, node, namespace) (
          sum by(cluster_type, cluster, node, namespace, pod) (
            max by(cluster_type, cluster, node, namespace, pod, container) (
              irate(container_network_transmit_bytes_total{}[40s])
            )
          )
          * on(cluster_type, cluster, node, namespace, pod)
          # pods with hostNetwork have node network statistics, it's meaningless
          kube_pod_info{host_network="false"}
        )
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: record-k8s-container-time
  labels:
    prometheus.io/instance: main
    instance.prometheus.io/main: enable
    instance.prometheus.io/prompp: enable
spec:
  groups:
  - name: record-k8s-namespace-network
    rules:
    - record: container_last_seen_time
      # this is a workaround for the issue that prometheus can't give you first and last timestamp of data vector
      # with this you can just use min_over_time(container_last_seen_time) and max_over_time(container_last_seen_time)
      expr: |-
        # when using timestamp() different series get different unique time values
        # this is inconvenient for the case of "sort result table by last_seen, and then sort by name"
        # using time() solves this issue by giving all series value of time of the recording rule evaluation
        # timestamp(container_start_time_seconds)
        container_start_time_seconds * 0 + time()
