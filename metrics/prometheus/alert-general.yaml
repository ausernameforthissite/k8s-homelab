---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: alert-general
  labels:
    prometheus.io/instance: main
    instance.prometheus.io/main: enable
    instance.prometheus.io/prompp: enable
spec:
  groups:
  - name: alert-general
    rules:
    - alert: TargetDown
      annotations:
        description: >-
          job {{ $labels.job }}:
          {{ $labels.instance }}
        summary: Target is unreachable
      expr: |-
        up{ok_to_be_missing!="true"} == 0
      for: 10m
      labels:
        severity: warning
    - alert: ServiceTargetsDown
      annotations:
        description: >-
          job {{ $labels.job }}:
          {{ $labels.namespace }}/{{ $labels.service }}:
          {{ $value | humanizePercentage }}
        summary: More than 10% of targets are unreachable
      expr: |-
        (
          count by (location, job, namespace, service) (up{namespace!="", service!=""} == 0)
          /
          count by (location, job, namespace, service) (up{namespace!="", service!=""})
        ) > 0
      for: 10m
      labels:
        severity: warning
    - alert: Watchdog
      annotations:
        description: 'This is an alert meant to ensure that the entire alerting pipeline is functional.
          This alert is always firing, therefore it should always be firing in Alertmanager
          and always fire against a receiver. There are integrations with various notification
          mechanisms that send a notification when this alert is not firing. For example the
          "DeadMansSnitch" integration in PagerDuty.
          '
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/general/watchdog
        summary: An alert that should always be firing to certify that Alertmanager is working properly.
      expr: vector(1)
      labels:
        severity: none
    - alert: InfoInhibitor
      annotations:
        description: 'This is an alert that is used to inhibit info alerts.
          By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with
          other alerts.
          This alert fires whenever there''s a severity="info" alert, and stops firing when another alert with a
          severity of ''warning'' or ''critical'' starts firing on the same namespace.
          This alert should be routed to a null receiver and configured to inhibit alerts with severity="info".
          '
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/general/infoinhibitor
        summary: Info-level alert inhibition.
      expr: ALERTS{severity = "info"} == 1 unless on (namespace) ALERTS{alertname != "InfoInhibitor", severity =~ "warning|critical", alertstate="firing"} == 1
      labels:
        severity: none
    - alert: MismatchedInstanceLocation
      annotations:
        description: >-
          {{ $labels.instance }}: {{ $value }}
        summary: There are several metrics from single instance that define different location value
      expr: count by(instance) (count by(location, instance) (up)) > 1
      labels:
        severity: info
    - alert: MismatchedInstanceLocationDetails
      annotations:
        description: >-
          job {{ $labels.job }}, location {{ $labels.original_location }}, instance {{ $labels.instance }}
        summary: More details for MismatchedInstanceLocation alert
      expr: |
        (count by(job, original_location, instance) (
          label_replace(label_replace(up, "original_location", "$1", "location", "(.*)"), "location", "", "location", ".*"))
        )
        and on(instance) 
        count by(instance) (group by(instance, location) (up)) > 1
      labels:
        severity: info
    - alert: ScrapeCloseToTimeout
      annotations:
        description: >-
          job {{ $labels.job }}, instance {{ $labels.instance }}:
          {{ $value | humanizePercentage }}
        summary: Scrape duration is too close to timeout
      expr: |
        # when scrape times out (duration/timeout == 1), we should instead receive "target down" alert
        (
          # trigger on peak above 90%
          (scrape_duration_seconds / scrape_timeout_seconds) > 0.9 < 1
        ) or (
          # trigger on average above 80%
          avg_over_time(scrape_duration_seconds[5m]) / scrape_timeout_seconds > 0.8 < 1
        ) or (
          # hysteresis: don't disable alert unless average falls below 0.6
          avg_over_time(scrape_duration_seconds[5m]) / scrape_timeout_seconds > 0.6 < 1
          and ignoring(alertname, alertstate, severity)
          ALERTS{alertname="ScrapeCloseToTimeout", alertstate="firing"}
        )
      for: 1m
      labels:
        severity: info
