
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: alert-ksm-pod-owner
  labels:
    prometheus.io/instance: main
    instance.prometheus.io/main: enable
    instance.prometheus.io/prompp: enable
spec:
  groups:
  - name: kubelet-k8s-pod-owner
    rules:
    - alert: UnknownControllerType
      annotations:
        description: >-
          {{ $labels.namespace }}/{{ $labels.pod }}:
          {{ $labels.owner_kind }}
        summary: kube_pod_owner{owner_kind} has unknown value
      expr: |-
        kube_pod_owner{
          # empty means pod is created manually
          owner_kind!="",
          owner_kind!="DaemonSet",
          owner_kind!="Job",
          # node means static pod
          owner_kind!="Node",
          owner_kind!="ReplicaSet",
          owner_kind!="StatefulSet",
          # Cluster is CNPG postgres cluster
          owner_kind!="Cluster",
          # OpenKruise controllers
          owner_kind!="BroadcastJob",
        }
      labels:
        severity: warning
    - alert: PodWithoutController
      annotations:
        description: >-
          {{ $labels.namespace }}/{{ $labels.pod }}
        summary: Pod without controller has been running for 24 hours
      expr: |-
        kube_pod_owner{owner_kind=""}
      # manually created pods are fine for short testing,
      # so let's delay this warning
      for: 24h
      labels:
        severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: alert-ksm-kube-app-status
  labels:
    prometheus.io/instance: main
    instance.prometheus.io/main: enable
    instance.prometheus.io/prompp: enable
spec:
  groups:
  - name: alert-ksm-kube-app-status
    rules:
    - alert: KubePodCrashLooping
      annotations:
        description: >-
          {{ $labels.namespace }}/{{ $labels.pod }}
        summary: Pod is in state CrashLoopBackOff
      expr: max_over_time(kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff"}[5m]) > 0
      for: 5m
      labels:
        severity: warning
    - alert: KubePodNotReady
      annotations:
        description: >-
          {{ $labels.namespace }}/{{ $labels.pod }}:
          {{ $labels.phase }}
        summary: Pod has been in a non-ready state for more than 5 minutes.
      expr: |-
        kube_pod_status_phase{phase=~"Pending|Unknown|Failed"} > 0
        unless on (cluster_type, cluster, namespace, pod)
        kube_pod_owner{owner_kind="Job"}
      for: 5m
      labels:
        severity: warning
    - alert: KubeDeploymentGenerationMismatch
      annotations:
        description: >-
          {{ $labels.namespace }}/deploy/{{ $labels.deployment }}
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentgenerationmismatch
        summary: Deployment generation mismatch due to possible roll-back
      expr: |-
        kube_deployment_status_observed_generation
          !=
        kube_deployment_metadata_generation
      for: 15m
      labels:
        severity: warning
    - alert: KubeDeploymentReplicasMismatch
      annotations:
        description: >-
          {{ $labels.namespace }}/deploy/{{ $labels.deployment }}
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentreplicasmismatch
        summary: Deployment has not matched the expected number of replicas for longer than 15 minutes
      expr: |-
        (
          kube_deployment_spec_replicas
            >
          kube_deployment_status_replicas_available
        ) and (
          changes(kube_deployment_status_replicas_updated[10m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
    - alert: KubeDeploymentRolloutStuck
      annotations:
        description: >-
          {{ $labels.namespace }}/deploy/{{ $labels.deployment }}
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentrolloutstuck
        summary: Deployment rollout is not progressing for longer than 15 minutes.
      expr: |-
        kube_deployment_status_condition{condition="Progressing", status="false"}
        != 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeStatefulSetReplicasMismatch
      annotations:
        description: >-
          {{ $labels.namespace }}/sts/{{ $labels.statefulset }}
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetreplicasmismatch
        summary: StatefulSet has not matched the expected number of replicas for longer than 15 minutes.
      expr: |-
        (
          kube_statefulset_status_replicas_ready
            !=
          kube_statefulset_status_replicas
        ) and (
          changes(kube_statefulset_status_replicas_updated[10m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
    - alert: KubeStatefulSetGenerationMismatch
      annotations:
        description: >-
          {{ $labels.namespace }}/sts/{{ $labels.statefulset }}
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetgenerationmismatch
        summary: StatefulSet generation mismatch due to possible roll-back
      expr: |-
        kube_statefulset_status_observed_generation
          !=
        kube_statefulset_metadata_generation
      for: 15m
      labels:
        severity: warning
    - alert: KubeStatefulSetUpdateNotRolledOut
      annotations:
        description: >-
          {{ $labels.namespace }}/sts/{{ $labels.statefulset }}
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetupdatenotrolledout
        summary: StatefulSet update has not been rolled out
      expr: |-
        (
          max by (namespace, statefulset, job, cluster) (
            kube_statefulset_status_current_revision
              unless
            kube_statefulset_status_update_revision
          )
            *
          (
            kube_statefulset_replicas
              !=
            kube_statefulset_status_replicas_updated
          )
        )  and (
          changes(kube_statefulset_status_replicas_updated[5m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
    - alert: KubeDaemonSetRolloutStuck
      annotations:
        description: >-
          {{ $labels.namespace }}/ds/{{ $labels.daemonset }}
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetrolloutstuck
        summary: DaemonSet rollout is stuck for at least 15m
      expr: |-
        (
          (
            kube_daemonset_status_current_number_scheduled
             !=
            kube_daemonset_status_desired_number_scheduled
          ) or (
            kube_daemonset_status_number_misscheduled
             !=
            0
          ) or (
            kube_daemonset_status_updated_number_scheduled
             !=
            kube_daemonset_status_desired_number_scheduled
          ) or (
            kube_daemonset_status_number_available
             !=
            kube_daemonset_status_desired_number_scheduled
          )
        ) and (
          changes(kube_daemonset_status_updated_number_scheduled[5m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
    - alert: KubeContainerWaiting
      annotations:
        description: >-
          {{ $labels.namespace }}/{{ $labels.pod }}/{{ $labels.container}}:
          reason: {{ $labels.reason }}
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontainerwaiting
        summary: Pod container waiting longer than 30 minutes
      expr: kube_pod_container_status_waiting_reason{reason!="CrashLoopBackOff"} > 0
      for: 30m
      labels:
        severity: warning
    - alert: KubeDaemonSetNotScheduled
      annotations:
        description: >-
          {{ $labels.namespace }}/ds/{{ $labels.daemonset }}:
          {{ $value }} pods missing
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetnotscheduled
        summary: DaemonSet pods are not scheduled
      expr: |-
        (
          kube_daemonset_status_desired_number_scheduled
          -
          kube_daemonset_status_current_number_scheduled
        ) > 0
      for: 10m
      labels:
        severity: warning
    - alert: KubeDaemonSetMisScheduled
      annotations:
        description: >-
          {{ $labels.namespace }}/ds/{{ $labels.daemonset }}:
          {{ $value }} excess pods
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetmisscheduled
        summary: DaemonSet pods are misscheduled
      expr: kube_daemonset_status_number_misscheduled > 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeJobNotCompleted
      annotations:
        description: >-
          {{ $labels.namespace }}/job/{{ $labels.job_name }}
        summary: Job did not complete in 12 hours
      expr: |-
        (
          time()
          -
          max by (namespace, job_name, cluster) (
            kube_job_status_start_time
            and
            kube_job_status_active > 0
          )
        ) > 12 * 60 * 60
      labels:
        severity: warning
    - alert: KubeJobFailed
      annotations:
        description: >-
          {{ $labels.namespace }}/job/{{ $labels.job_name }}
        summary: Job failed to complete
      expr: kube_job_failed > 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeHpaReplicasMismatch
      annotations:
        description: >-
          {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }}
        summary: HPA has not matched desired number of replicas for longer than 15 minutes
      expr: |-
        (
          kube_horizontalpodautoscaler_status_desired_replicas
          !=
          kube_horizontalpodautoscaler_status_current_replicas
        )
          and
        (
          kube_horizontalpodautoscaler_status_current_replicas
          >
          kube_horizontalpodautoscaler_spec_min_replicas
        )
          and
        (
          kube_horizontalpodautoscaler_status_current_replicas
          <
          kube_horizontalpodautoscaler_spec_max_replicas
        )
          and
        changes(kube_horizontalpodautoscaler_status_current_replicas[15m]) == 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeHpaMaxedOut
      annotations:
        description: >-
          {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }}
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpamaxedout
        summary: HPA is running at max replicas for longer than 15 minutes
      expr: |-
        kube_horizontalpodautoscaler_status_current_replicas
          ==
        kube_horizontalpodautoscaler_spec_max_replicas
      for: 15m
      labels:
        severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: alert-ksm-kube-resources
  labels:
    prometheus.io/instance: main
    instance.prometheus.io/main: enable
    instance.prometheus.io/prompp: enable
spec:
  groups:
  - name: alert-ksm-kube-resources
    rules:
    - alert: KubeCPUOvercommit
      annotations:
        description: >-
          {{ $value }} CPU overcommit
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecpuovercommit
        summary: Cluster has overcommitted CPU resource requests and cannot tolerate node failure
      expr: |-
        # check that cluster without the biggest node is overcommitted
        sum by (cluster_type, cluster) (node_namespace:kube_pod_container_resource_requests{resource="cpu"})
        -
        (
          sum by (cluster_type, cluster) (kube_node_status_allocatable{resource="cpu"})
          -
          max by (cluster_type, cluster) (kube_node_status_allocatable{resource="cpu"})
        )
        > 0
        and
        # check that cluster has more than one node
        (
          sum by (cluster_type, cluster) (kube_node_status_allocatable{resource="cpu"})
          -
          max by (cluster_type, cluster) (kube_node_status_allocatable{resource="cpu"})
        ) > 0
      for: 10m
      labels:
        severity: warning
    - alert: KubeMemoryOvercommit
      annotations:
        description: >-
          {{ $value | humanize1024 }} memory overcommit
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryovercommit
        summary: Cluster has overcommitted memory resource requests and cannot tolerate node failure.
      expr: |-
        # check that cluster without the biggest noe is overcommitted
        sum by (cluster_type, cluster) (node_namespace:kube_pod_container_resource_requests{resource="memory"})
        -
        (
          sum by (cluster_type, cluster) (kube_node_status_allocatable{resource="memory"})
          -
          max by (cluster_type, cluster) (kube_node_status_allocatable{resource="memory"})
        )
        > 0
        and
        # check that cluster has more than one node
        (
          sum by (cluster_type, cluster) (kube_node_status_allocatable{resource="memory"})
          -
          max by (cluster_type, cluster) (kube_node_status_allocatable{resource="memory"})
        ) > 0
      for: 10m
      labels:
        severity: warning
    - alert: KubeCPUQuotaOvercommit
      annotations:
        description: >-
          {{ $value | humanizePercentage }} CPU overcommit
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecpuquotaovercommit
        summary: Cluster has overcommitted CPU Namespaces resource requests
      expr: |-
        sum by (cluster_type, cluster) (
          min without(resource) (kube_resourcequota{type="hard", resource=~"(cpu|requests.cpu)"})
        ) / sum by (cluster_type, cluster) (kube_node_status_allocatable{resource="cpu"})
        > 1.5
      for: 5m
      labels:
        severity: warning
    - alert: KubeMemoryQuotaOvercommit
      annotations:
        description: >-
          {{ $value | humanizePercentage }} memory overcommit
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryquotaovercommit
        summary: Cluster has overcommitted memory Namespaces resource requests
      expr: |-
        sum by (cluster_type, cluster) (
          min without(resource) (kube_resourcequota{type="hard", resource=~"(memory|requests.memory)"})
        ) /
        sum by (cluster_type, cluster) (kube_node_status_allocatable{resource="memory"})
        > 1.5
      for: 5m
      labels:
        severity: warning
    - alert: KubeQuotaAlmostFull
      annotations:
        description: >-
          Namespace {{ $labels.namespace }}:
          {{ $value | humanizePercentage }} of {{ $labels.resource }} quota
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotaalmostfull
        summary: Namespace has used over 90% of its quota
      expr: |-
        kube_resourcequota{type="used"}
          / ignoring(type)
        (kube_resourcequota{type="hard"} > 0)
        > 0.9 < 1
      for: 15m
      labels:
        severity: info
    - alert: KubeQuotaFullyUsed
      annotations:
        description: >-
          Namespace {{ $labels.namespace }}:
          {{ $value | humanizePercentage }} of {{ $labels.resource }} quota
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotafullyused
        summary: Namespace quota is fully used
      expr: |-
        kube_resourcequota{type="used"}
          / ignoring(type)
        (kube_resourcequota{type="hard"} > 0)
          == 1
      for: 15m
      labels:
        severity: info
    - alert: KubeQuotaExceeded
      annotations:
        description: >-
          Namespace {{ $labels.namespace }}:
          {{ $value | humanizePercentage }} of {{ $labels.resource }} quota
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotaexceeded
        summary: Namespace quota has exceeded the limits
      expr: |-
        kube_resourcequota{type="used"}
          / ignoring(type)
        (kube_resourcequota{type="hard"} > 0)
          > 1
      for: 15m
      labels:
        severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: alert-ksm-kube-system
  labels:
    prometheus.io/instance: main
    instance.prometheus.io/main: enable
    instance.prometheus.io/prompp: enable
spec:
  groups:
  - name: alert-ksm-kube-system
    rules:
    - alert: KubeNodeNotReady
      annotations:
        description: >-
          {{ $labels.node }}
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodenotready
        summary: Node is not ready for more than 15 minutes
      expr: kube_node_status_condition{condition="Ready", status="true"} == 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeNodeUnreachable
      annotations:
        description: >-
          {{ $labels.node }}
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodeunreachable
        summary: Node is unreachable
      expr: |-
        (
          kube_node_spec_taint{key="node.kubernetes.io/unreachable",effect="NoSchedule"}
          unless ignoring(key, value)
          kube_node_spec_taint{key=~"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn"}
        ) == 1
      for: 15m
      labels:
        severity: warning
    - alert: KubeletTooManyPods
      annotations:
        description: >-
          {{ $labels.node }}:
          {{ $value | humanizePercentage }}
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubelettoomanypods
        summary: Kubelet is running above 95% of its pod capacity
      expr: |-
        count by (cluster_type, cluster, node) (
          (kube_pod_status_phase{phase="Running"} == 1)
          * on (cluster_type, cluster, pod, namespace)
          group_left(node)
          topk by (cluster_type, cluster, pod, namespace) (1, kube_pod_info)
        )
        /
        max by (cluster_type, cluster, node) (
          kube_node_status_capacity{resource="pods"} != 1
        ) > 0.95
      for: 15m
      labels:
        severity: info
    - alert: KubeNodeReadinessFlapping
      annotations:
        description: >-
          {{ $labels.node }}:
          {{ $value }} changes
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodereadinessflapping
        summary: Node readiness changed more than 2 times over the last 15 minutes
      expr: |-
        sum by (cluster_type, cluster, node) (
          changes(kube_node_status_condition{status="true", condition="Ready"}[15m])
        ) > 2
      for: 15m
      labels:
        severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: alert-ksm-container-spec-mismatch
  labels:
    prometheus.io/instance: main
    instance.prometheus.io/main: enable
    instance.prometheus.io/prompp: enable
spec:
  groups:
  - name: alert-ksm-container-spec-mismatch
    rules:
    - alert: KubeContainerCpuLimitMismatch
      annotations:
        description: >-
          {{ $labels.namespace }}/{{ $labels.pod }}/{{ $labels.container }}:
          {{ $value | humanize }} times
        summary: Container real CPU limit does not match k8s spec. Note that CPU limit is capped at minimum 10m
      expr: |-
        max by(cluster_type, cluster, namespace, pod, container) (container_spec_cpu_quota / container_spec_cpu_period)
        /
        max by(cluster_type, cluster, namespace, pod, container) (kube_pod_container_resource_limits{resource="cpu"})
        != 1
      for: 5m
      labels:
        severity: warning
    - alert: KubeContainerMemoryLimitMismatch
      annotations:
        description: >-
          {{ $labels.namespace }}/{{ $labels.pod }}/{{ $labels.container }}:
          {{ $value | humanize }} times
        summary: Container real memory limit does not match k8s spec
      expr: |-
        max by(cluster_type, cluster, namespace, pod, container) (container_spec_memory_limit_bytes)
        /
        max by(cluster_type, cluster, namespace, pod, container) (kube_pod_container_resource_limits{resource="memory"})
        != 1
      for: 5m
      labels:
        severity: warning
